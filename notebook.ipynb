{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xD5ytEyLn5uQ"
      },
      "source": [
        "Assingment 3 - NLP 201 - Parsa Mazaheri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0LpVJJmn5uT",
        "outputId": "4d7696b8-00ab-444f-9837-790ab2fd8270"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fU_Mco6Vn5uV"
      },
      "outputs": [],
      "source": [
        "def evaluate(test_sentences, tagged_test_sentences):\n",
        "    gold = [str(tag) for sentence in test_sentences for token, tag in sentence]\n",
        "    pred = [str(tag) for sentence in tagged_test_sentences for token, tag in sentence]\n",
        "    print(metrics.classification_report(gold, pred, zero_division=0))\n",
        "\n",
        "\n",
        "def get_token_tag_tuples(sent):\n",
        "    return [nltk.tag.str2tuple(t) for t in sent.split()]\n",
        "\n",
        "\n",
        "def get_tagged_sentences(text):\n",
        "    sentences = []\n",
        "\n",
        "    blocks = text.split(\"======================================\")\n",
        "    for block in blocks:\n",
        "        sents = block.split(\"\\n\\n\")\n",
        "        for sent in sents:\n",
        "            sent = sent.replace(\"\\n\", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n",
        "            if sent != \"\":\n",
        "                sentences.append(sent)\n",
        "    return sentences\n",
        "\n",
        "\n",
        "def load_treebank_splits(datadir):\n",
        "    train = []\n",
        "    dev = []\n",
        "    test = []\n",
        "\n",
        "    print(\"Loading treebank data...\")\n",
        "\n",
        "    for subdir, dirs, files in os.walk(datadir):\n",
        "        for filename in files:\n",
        "            if filename.endswith(\".pos\"):\n",
        "                filepath = subdir + os.sep + filename\n",
        "                with open(filepath, \"r\") as fh:\n",
        "                    text = fh.read()\n",
        "                    if int(subdir.split(os.sep)[-1]) in range(0, 19):\n",
        "                        train += get_tagged_sentences(text)\n",
        "\n",
        "                    if int(subdir.split(os.sep)[-1]) in range(19, 22):\n",
        "                        dev += get_tagged_sentences(text)\n",
        "\n",
        "                    if int(subdir.split(os.sep)[-1]) in range(22, 25):\n",
        "                        test += get_tagged_sentences(text)\n",
        "\n",
        "    print(\"Train set size: \", len(train))\n",
        "    print(\"Dev set size: \", len(dev))\n",
        "    print(\"Test set size: \", len(test))\n",
        "\n",
        "    return train, dev, test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4sNnJS1n5uX"
      },
      "source": [
        "##### read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3jbqU3zoRWs"
      },
      "outputs": [],
      "source": [
        "# drop data.zip into colab\n",
        "!unzip /content/data.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbHCl23kn5uX",
        "outputId": "e4ec10c0-058c-4bf4-bfaa-81d573fa61f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading treebank data...\n",
            "Train set size:  51681\n",
            "Dev set size:  7863\n",
            "Test set size:  9046\n"
          ]
        }
      ],
      "source": [
        "datadir = r'/content/data/penn-treeban3-wsj/wsj'\n",
        "# r'C:\\Users\\Parsa Mazaheri\\Desktop\\Canvas\\NLP 201\\HW\\HW3\\data\\penn-treeban3-wsj\\wsj'\n",
        "# os.path.join(\"data\", \"penn-treebank3-wsj\", \"wsj\")\n",
        "    \n",
        "train, dev, test = load_treebank_splits(datadir)\n",
        "\n",
        "train_sentences = [get_token_tag_tuples(sent) for sent in train]\n",
        "dev_sentences = [get_token_tag_tuples(sent) for sent in dev]\n",
        "test_sentences = [get_token_tag_tuples(sent) for sent in test]\n",
        "\n",
        "tagged_test_sentences = [nltk.pos_tag([token for token, tag in sentence]) for sentence in test_sentences]\n",
        "# evaluate(test_sentences, tagged_test_sentences, zero_division=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BHNNtKRn5uZ",
        "outputId": "b44ca265-dddd-4f3a-e5a1-84dac1a5c7e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Under/IN  these/DT deals/NNS ,/,  the/DT RTC/NNP sells/VBZ just/RB  the/DT deposits/NNS and/CC  the/DT healthy/JJ assets/NNS ./. \n",
            "sentence:\n",
            "['Under', 'these', 'deals', ',', 'the', 'RTC', 'sells', 'just', 'the', 'deposits', 'and', 'the', 'healthy', 'assets', '.']\n",
            "ground truth:\n",
            "[('Under', 'IN'), ('these', 'DT'), ('deals', 'NNS'), (',', ','), ('the', 'DT'), ('RTC', 'NNP'), ('sells', 'VBZ'), ('just', 'RB'), ('the', 'DT'), ('deposits', 'NNS'), ('and', 'CC'), ('the', 'DT'), ('healthy', 'JJ'), ('assets', 'NNS'), ('.', '.')]\n",
            "predicted:\n",
            "[('Under', 'IN'), ('these', 'DT'), ('deals', 'NNS'), (',', ','), ('the', 'DT'), ('RTC', 'NNP'), ('sells', 'VBZ'), ('just', 'RB'), ('the', 'DT'), ('deposits', 'NNS'), ('and', 'CC'), ('the', 'DT'), ('healthy', 'JJ'), ('assets', 'NNS'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "k = 10\n",
        "\n",
        "print(test[k])\n",
        "\n",
        "print('sentence:')\n",
        "print([token for token, tag in test_sentences[k]])\n",
        "\n",
        "print('ground truth:')\n",
        "print(test_sentences[k])\n",
        "\n",
        "print(\"predicted:\")\n",
        "print(tagged_test_sentences[k])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b95bFS16n5ua"
      },
      "source": [
        "##### Hidden Markov Model + Viterbi Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mujruD0Kn5ub"
      },
      "outputs": [],
      "source": [
        "LOW_PROB = 1e-10; \n",
        "\n",
        "def get_prob(prob):\n",
        "    return np.log(prob) if prob != 0.0 else np.log(LOW_PROB)\n",
        "    \n",
        "\n",
        "class HMM:\n",
        "    def __init__(self):\n",
        "        self.transitions = {}\n",
        "        self.emissions = {}\n",
        "        self.words = {}\n",
        "        self.tags = {}  \n",
        "        \n",
        "\n",
        "    @staticmethod\n",
        "    def add_start_stop(sentence):\n",
        "        return [(\"<start>\", \"START\")]+ sentence + [(\"<stop>\", \"STOP\")]\n",
        "\n",
        "\n",
        "    def get_words_and_tags(self, sentences):\n",
        "        words, tags = {}, {}\n",
        "        for sentence in sentences:\n",
        "            for token, tag in sentence:\n",
        "                tags[tag] = tags.get(tag, 0) + 1\n",
        "                words[token] = words.get(token, 0) + 1\n",
        "        return words, tags\n",
        "\n",
        "    \n",
        "    def get_transition_matrix(self, sentences, alpha=0.0):\n",
        "        print(\"> Building transition matrix ...\")\n",
        "        # Initialize the transition counts.\n",
        "        transitions = defaultdict(lambda: 0)\n",
        "        N = len(self.tags)\n",
        "\n",
        "        # Count the transitions (bigrams).\n",
        "        for sentence in sentences:\n",
        "            for i in range(len(sentence) - 1):\n",
        "                t1, t2 = sentence[i][1], sentence[i + 1][1]\n",
        "                transitions[(t1, t2)] += 1\n",
        "            \n",
        "        \n",
        "        # Calculate the transition probabilities.\n",
        "        for (tag1, tag2), count in transitions.items():\n",
        "            transitions[(tag1, tag2)] = (count + alpha) / (self.tags[tag1] + alpha * N)\n",
        "\n",
        "        return transitions\n",
        "\n",
        "    \n",
        "    def get_emission_matrix(self, sentences, alpha=0.0):\n",
        "        print(\"> Building emission matrix ...\")\n",
        "        # Initialize the emission counts.\n",
        "        emissions = defaultdict(lambda: 0)\n",
        "        emissions_count = defaultdict(lambda: 0)\n",
        "        N = len(self.words)\n",
        "\n",
        "        # Count the emissions (bigrams).\n",
        "        for sentence in sentences:\n",
        "            for token, tag in sentence:\n",
        "                emissions_count[(token, tag)] += 1\n",
        "        \n",
        "        # Calculate the emission probabilities.\n",
        "        for (token, tag), count in emissions_count.items():\n",
        "            emissions[(token, tag)] = (count + alpha) / (self.tags[tag] + alpha * N)\n",
        "\n",
        "        return emissions\n",
        "\n",
        "   \n",
        "    def train(self, sentences, alpha=1):\n",
        "        print(\"Training ...\")\n",
        "        # training: get tags, words, and build transition and emission matrices\n",
        "        sentences = [self.add_start_stop(sentence) for sentence in sentences]\n",
        "\n",
        "        self.words, self.tags = self.get_words_and_tags(sentences)\n",
        "        self.transitions = self.get_transition_matrix(sentences, alpha)\n",
        "        self.emissions = self.get_emission_matrix(sentences, alpha)\n",
        "\n",
        "        print(\"> Done\")\n",
        "\n",
        "    \n",
        "    def viterbi(self, sentence):\n",
        "        \"\"\"\n",
        "        Viterbi algorithm for finding the most likely sequence of tags\n",
        "        given a sequence of observations.\n",
        "        \"\"\"\n",
        "        # Initialize the viterbi matrix and backpointer matrix.\n",
        "        tags = list(self.tags.keys())\n",
        "        viterbi = np.zeros((len(tags), len(sentence)))\n",
        "        backpointer = np.zeros((len(tags), len(sentence)))\n",
        "\n",
        "        # calculate the probs for the sentence \n",
        "        for w in range(1, len(sentence)):\n",
        "            for t in range(len(tags)):\n",
        "                # dynamic programming: calculate the total prob of the prev tag and the current tag\n",
        "                total_prob, prev_tag = -1e8, 0\n",
        "                for j in range(len(tags)):\n",
        "                    prob = viterbi[j, w-1] + \\\n",
        "                        get_prob(self.transitions[(tags[j], tags[t])]) + \\\n",
        "                        get_prob(self.emissions[(sentence[w][0], tags[t])])\n",
        "                    \n",
        "                    if prob > total_prob:\n",
        "                        total_prob = prob\n",
        "                        prev_tag = j\n",
        "                \n",
        "                backpointer[t, w] = prev_tag\n",
        "                viterbi[t, w] = total_prob\n",
        "\n",
        "        # Find the most likely sequence of tags.\n",
        "        path = []\n",
        "        for j in range(len(sentence)-1, -1, -1):\n",
        "            best_choice = np.argmax(viterbi[:, j])\n",
        "            path.append(\n",
        "                (sentence[j][0], tags[best_choice])\n",
        "            )\n",
        "        path = path[1:-1]   # remove <start> and <stop>\n",
        "        return path[::-1]   # reverse the path\n",
        "            \n",
        "\n",
        "    # predict the tags for the sentences\n",
        "    def predict(self, sentences):\n",
        "        print(\"Predicting ...\")\n",
        "        predicted = []\n",
        "        sentences = [self.add_start_stop(sentence) for sentence in sentences]\n",
        "        for sentence in tqdm(sentences):\n",
        "            predicted.append(self.viterbi(sentence))\n",
        "        return predicted\n",
        "\n",
        "\n",
        "    # evaluate the model\n",
        "    def evaluate(self, gold_sentences, predicted_sentences):\n",
        "        print(\"Evaluating ...\")\n",
        "        gold = [str(tag) for sentence in gold_sentences for token, tag in sentence]\n",
        "        pred = [str(tag) for sentence in predicted_sentences for token, tag in sentence]\n",
        "        return metrics.classification_report(gold, pred, zero_division=0)\n",
        "\n",
        "    \n",
        "    # get the confusion matrix\n",
        "    def get_confusion_matrix(self, gold_sentence, predicted_sentence):\n",
        "        print(\"Confusion Matrix ...\")\n",
        "        gold = [str(tag) for token, tag in gold_sentence]\n",
        "        pred = [str(tag) for token, tag in predicted_sentence]\n",
        "        return metrics.confusion_matrix(gold, pred)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUPGe656n5uc",
        "outputId": "c6ad7b43-a1b3-4807-9380-0d65d44c0f37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training ...\n",
            "> Building transition matrix ...\n",
            "> Building emission matrix ...\n",
            "> Done\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "hmm = HMM()\n",
        "hmm.train(train_sentences, alpha=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "C1URj7QSn5uc"
      },
      "outputs": [],
      "source": [
        "# mode \n",
        "mode = 'test'\n",
        "\n",
        "if mode == 'dev':\n",
        "    sentences = dev_sentences[0:20]\n",
        "elif mode == 'test':    \n",
        "    sentences = test_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHiqCf23n5ud",
        "outputId": "5487c5be-9c3a-451b-ab00-0d06eeec4b0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicting ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9046/9046 [47:12<00:00,  3.19it/s]\n"
          ]
        }
      ],
      "source": [
        "# predict the test sentences\n",
        "predicted = hmm.predict(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhxcffWCn5ud",
        "outputId": "fc3763b0-3e98-427f-f4f5-34f24bafe8b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating ...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           #       1.00      0.91      0.95        22\n",
            "           $       1.00      1.00      1.00      1138\n",
            "          ''       1.00      0.99      1.00      1423\n",
            "           (       1.00      1.00      1.00       249\n",
            "           )       1.00      1.00      1.00       252\n",
            "           ,       1.00      1.00      1.00      9056\n",
            "           .       1.00      1.00      1.00      7035\n",
            "           :       1.00      1.00      1.00       983\n",
            "          CC       0.99      1.00      1.00      4289\n",
            "          CD       0.99      0.93      0.96      6023\n",
            "          DT       0.92      0.99      0.96     14946\n",
            "          EX       0.96      0.45      0.62       174\n",
            "          FW       0.60      0.08      0.14        38\n",
            "          IN       0.91      0.99      0.95     18147\n",
            "          JJ       0.88      0.87      0.87     10704\n",
            "         JJR       0.68      0.91      0.78       581\n",
            "     JJR|RBR       0.00      0.00      0.00         4\n",
            "         JJS       0.81      0.95      0.87       374\n",
            "       JJ|IN       0.00      0.00      0.00         1\n",
            "          LS       0.00      0.00      0.00        15\n",
            "          MD       1.00      0.99      1.00      1674\n",
            "          NN       0.92      0.96      0.94     23468\n",
            "         NNP       0.95      0.91      0.93     17236\n",
            "        NNPS       0.34      0.05      0.09       239\n",
            "         NNS       0.98      0.95      0.96     10697\n",
            "     NNS|VBZ       0.00      0.00      0.00         0\n",
            "         PDT       0.00      0.00      0.00        66\n",
            "         POS       0.98      1.00      0.99      1638\n",
            "         PRP       0.96      0.99      0.98      2930\n",
            "        PRP$       0.99      1.00      0.99      1433\n",
            "          RB       0.88      0.85      0.87      5853\n",
            "         RBR       0.68      0.07      0.12       382\n",
            "     RBR|JJR       0.00      0.00      0.00         2\n",
            "         RBS       0.60      0.07      0.12        87\n",
            "       RB|JJ       0.00      0.00      0.00         2\n",
            "          RP       0.00      0.00      0.00       356\n",
            "        STOP       0.00      0.00      0.00         0\n",
            "         SYM       1.00      0.27      0.43        11\n",
            "          TO       1.00      1.00      1.00      3899\n",
            "          UH       1.00      0.25      0.40        20\n",
            "          VB       0.92      0.94      0.93      4766\n",
            "         VBD       0.91      0.93      0.92      5869\n",
            "         VBG       0.96      0.81      0.88      2592\n",
            "      VBG|NN       0.00      0.00      0.00         1\n",
            "         VBN       0.87      0.76      0.81      3580\n",
            "      VBN|JJ       0.00      0.00      0.00         2\n",
            "         VBP       0.95      0.85      0.90      2209\n",
            "         VBZ       0.97      0.94      0.96      3608\n",
            "         WDT       1.00      0.54      0.70       773\n",
            "          WP       0.99      1.00      0.99       397\n",
            "         WP$       1.00      1.00      1.00        47\n",
            "         WRB       1.00      0.99      1.00       425\n",
            "          ``       1.00      1.00      1.00      1422\n",
            "\n",
            "    accuracy                           0.94    171138\n",
            "   macro avg       0.73      0.64      0.66    171138\n",
            "weighted avg       0.94      0.94      0.94    171138\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# evaluate the model\n",
        "results = hmm.evaluate(sentences, predicted)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbqBvLkZn5ue",
        "outputId": "60d15b8e-e185-4ea2-d25f-1814b24857f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix ...\n",
            "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 3 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 4 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 4 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 3 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 3 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n"
          ]
        }
      ],
      "source": [
        "# confusion matrix\n",
        "\n",
        "confusion_matrix = hmm.get_confusion_matrix(sentences[0], predicted[0])\n",
        "print(confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFdxhI-gn5uf"
      },
      "outputs": [],
      "source": [
        "# nltk_prediction = [nltk.pos_tag([token for token, tag in sentence]) for sentence in sentences]\n",
        "# evaluate(sentences, nltk_prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZeUtUvan5uf"
      },
      "source": [
        "##### BaseModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ATfS6hFRn5uf"
      },
      "outputs": [],
      "source": [
        "class BaseModel:\n",
        "    def __init__(self):\n",
        "        self.words_freq = {}\n",
        "        self.words_tag_freq = {}\n",
        "    \n",
        "    @staticmethod\n",
        "    def add_start_stop(sentence):\n",
        "        return [(\"<start>\", \"START\")]+ sentence + [(\"<stop>\", \"STOP\")]\n",
        "\n",
        "    def get_words_and_tags(self, sentences):\n",
        "        words_freq, words_tag_freq = {}, {}\n",
        "        # count the number of times each word appears with each tag\n",
        "        for sentence in sentences:\n",
        "            for word, tag in sentence:\n",
        "                if word in words_freq:\n",
        "                    if tag in words_freq[word]:\n",
        "                        words_freq[word][tag] += 1\n",
        "                    else:\n",
        "                        words_freq[word][tag] = 1\n",
        "                else:\n",
        "                    words_freq[word] = {tag: 1}\n",
        "        \n",
        "        # get the most frequent tag for each word\n",
        "        for word, tag_freq in words_freq.items():\n",
        "            words_tag_freq[word] = max(tag_freq, key=tag_freq.get)\n",
        "\n",
        "        return words_freq, words_tag_freq\n",
        "\n",
        "\n",
        "    def train(self, sentences):\n",
        "        print(\"Training ...\")\n",
        "        sentences = [self.add_start_stop(sentence) for sentence in sentences]\n",
        "        self.words_freq, self.words_tag_freq = self.get_words_and_tags(sentences)\n",
        "        print(\"> Done\")\n",
        "\n",
        "    \n",
        "    def get_prediction(self, sentence):\n",
        "        pred = []\n",
        "        for word, tag in sentence:\n",
        "            if word in self.words_tag_freq:\n",
        "                pred.append((word, self.words_tag_freq[word]))\n",
        "            else:\n",
        "                pred.append((word, \"<unk>\"))\n",
        "        return pred[1:-1]   # remove <start> and <stop>\n",
        "\n",
        "\n",
        "    def predict(self, sentences):\n",
        "        print(\"Predicting ...\")\n",
        "        predicted = []\n",
        "        sentences = [self.add_start_stop(sentence) for sentence in sentences]\n",
        "        \n",
        "        for sentence in tqdm(sentences):\n",
        "            predicted.append(self.get_prediction(sentence))\n",
        "        return predicted\n",
        "\n",
        "\n",
        "    def evaluate(self, gold_sentences, predicted_sentences):\n",
        "        gold = [str(tag) for sentence in gold_sentences for token, tag in sentence]\n",
        "        pred = [str(tag) for sentence in predicted_sentences for token, tag in sentence]\n",
        "        print(metrics.classification_report(gold, pred, zero_division=0))\n",
        "    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7WmStCqn5ug",
        "outputId": "42f327c3-5b4a-473f-a3b7-c5a0004bd485"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training ...\n",
            "> Done\n",
            "Predicting ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9046/9046 [00:00<00:00, 107651.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           #       1.00      1.00      1.00        22\n",
            "           $       1.00      1.00      1.00      1138\n",
            "          ''       1.00      0.99      1.00      1423\n",
            "           (       1.00      1.00      1.00       249\n",
            "           )       1.00      1.00      1.00       252\n",
            "           ,       1.00      1.00      1.00      9056\n",
            "           .       1.00      1.00      1.00      7035\n",
            "           :       1.00      1.00      1.00       983\n",
            "       <unk>       0.00      0.00      0.00         0\n",
            "          CC       1.00      1.00      1.00      4289\n",
            "          CD       0.99      0.90      0.94      6023\n",
            "          DT       0.99      0.99      0.99     14946\n",
            "          EX       0.89      1.00      0.94       174\n",
            "          FW       0.35      0.24      0.28        38\n",
            "          IN       0.94      0.98      0.96     18147\n",
            "          JJ       0.88      0.86      0.87     10704\n",
            "         JJR       0.66      0.95      0.78       581\n",
            "     JJR|RBR       0.00      0.00      0.00         4\n",
            "         JJS       0.79      0.95      0.86       374\n",
            "       JJ|IN       0.00      0.00      0.00         1\n",
            "          LS       0.00      0.00      0.00        15\n",
            "          MD       0.99      1.00      0.99      1674\n",
            "          NN       0.93      0.92      0.93     23468\n",
            "         NNP       0.97      0.86      0.91     17236\n",
            "        NNPS       0.27      0.47      0.35       239\n",
            "         NNS       0.97      0.94      0.96     10697\n",
            "     NNS|VBZ       0.00      0.00      0.00         0\n",
            "         PDT       0.00      0.00      0.00        66\n",
            "         POS       0.87      1.00      0.93      1638\n",
            "         PRP       1.00      0.99      1.00      2930\n",
            "        PRP$       0.99      1.00      0.99      1433\n",
            "          RB       0.90      0.86      0.88      5853\n",
            "         RBR       0.83      0.24      0.37       382\n",
            "     RBR|JJR       0.00      0.00      0.00         2\n",
            "         RBS       0.40      0.02      0.04        87\n",
            "       RB|JJ       0.00      0.00      0.00         2\n",
            "          RP       0.34      0.12      0.17       356\n",
            "         SYM       0.80      0.73      0.76        11\n",
            "          TO       1.00      1.00      1.00      3899\n",
            "          UH       1.00      0.50      0.67        20\n",
            "          VB       0.79      0.70      0.74      4766\n",
            "         VBD       0.87      0.84      0.85      5869\n",
            "         VBG       0.90      0.85      0.87      2592\n",
            "      VBG|NN       0.00      0.00      0.00         1\n",
            "         VBN       0.73      0.72      0.72      3580\n",
            "      VBN|JJ       0.00      0.00      0.00         2\n",
            "         VBP       0.76      0.71      0.73      2209\n",
            "         VBZ       0.95      0.87      0.91      3608\n",
            "         WDT       1.00      0.54      0.70       773\n",
            "          WP       0.99      1.00      0.99       397\n",
            "         WP$       1.00      1.00      1.00        47\n",
            "         WRB       1.00      0.99      1.00       425\n",
            "          ``       1.00      1.00      1.00      1422\n",
            "\n",
            "    accuracy                           0.91    171138\n",
            "   macro avg       0.71      0.67      0.68    171138\n",
            "weighted avg       0.94      0.91      0.92    171138\n",
            "\n"
          ]
        }
      ],
      "source": [
        "base = BaseModel()\n",
        "\n",
        "base.train(train_sentences)\n",
        "predicted = base.predict(sentences)\n",
        "base.evaluate(sentences, predicted)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "1099c6f1fc175e7e045cb4ce78c8a3045efc9451e7ca13edede79d44aa010a87"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
